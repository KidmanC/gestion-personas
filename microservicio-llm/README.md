# ğŸ“ **README â€“ Servicio RAG con HuggingFace Router**

## ğŸ“Œ **DescripciÃ³n del Proyecto**

Este servicio implementa un **sistema RAG (Retrieval-Augmented Generation)** que:

- Consulta un microservicio externo para obtener personas (opcional).
- Procesa una pregunta del usuario.
- Llama a un modelo LLM a travÃ©s del **HuggingFace Inference Router**.
- Retorna una respuesta generada por el modelo, usando modo sin pensamiento interno cuando es compatible.

DiseÃ±ado para ser **simple**, **modular**, fÃ¡cil de contenerizar en **Docker**, y usable en proyectos acadÃ©micos o demostrativos sin necesidad de proveedores con billing activo.

---

## ğŸš€ **Endpoints Principales**

### **GET /check_llm**
Prueba simple para verificar la conexiÃ³n con el modelo LLM.


Ejemplo de peticiÃ³n:

```json
{
  "consulta": "Â¿QuÃ© fecha es hoy?"
}
```

Ejemplo de respuesta:

```json
{
  "personas": [],
  "answer": "28 de marzo de 2025",
  "raw": {...}
}
```


## ğŸ”¹ **POST /rag**

### **DescripciÃ³n**
Endpoint principal usado por el **frontend**.  
Procesa la consulta del usuario, obtiene los datos del microservicio de personas y devuelve **solo la lista mapeada**, lista para ser consumida por React.

### **Body esperado**
```json
{
  "consulta": "texto de la consulta"
}
```

### **Respuesta**
Lista de personas mapeadas:

```json
[
  {
    "id": 1,
    "nombre": "Juan PÃ©rez",
    "edad": 30
  }
]
```

Si no hay personas:

```json
[]
```

### **Errores**
- `500` error interno si falla el RAG o el microservicio.

---

## ğŸ”¹ **POST /rag_full**

### **DescripciÃ³n**
Endpoint completo de depuraciÃ³n (**solo para pruebas**).  
Devuelve:

- Personas obtenidas  
- Respuesta final del LLM  
- Respuesta cruda del proveedor (HuggingFace Router)

Sirve para verificar que el modelo estÃ¡ respondiendo correctamente, que el pipeline estÃ¡ funcionando y que la integraciÃ³n con HuggingFace Router es correcta.

### **Body esperado**
```json
{
  "consulta": "texto de la consulta"
}
```

### **Respuesta**
```json
{
  "personas": [
    { "id": 1, "nombre": "Juan PÃ©rez" }
  ],
  "answer": "La respuesta procesada del modelo",
  "raw": { ... respuesta original del modelo ... }
}
```

### **Errores**
- `500` si algÃºn componente del pipeline falla.

---

## ğŸ“ Notas adicionales

- `/rag` debe ser usado por el frontend en producciÃ³n.
- `/rag_full` es solo para desarrolladores (debug).
- Ambos endpoints esperan el mismo modelo `QueryBody`.

---

## ğŸ›  **Requisitos**

- Docker
- Archivo `.env` ubicado en la raÃ­z del proyecto

---

# ğŸ“¦ **Instrucciones de Docker**

## ğŸ”¨ **1. Construir la imagen**

```bash
docker build -t servicio-rag .
```

## â–¶ï¸ **2. Ejecutar el contenedor con variables de entorno**

```bash
docker run --env-file .env -p 8000:8000 servicio-rag
```

Esto levantarÃ¡ el servicio en:

ğŸ‘‰ http://localhost:8000  
ğŸ‘‰ http://localhost:8000/docs

---

# ğŸ§© **Formato del archivo `.env`**

```
PERSONS_SERVICE_URL=http://host.docker.internal:3002/persons
GEMINI_API_URL=https://router.huggingface.co/v1/chat/completions
GEMINI_API_KEY=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXX
GEMINI_MODEL=openai/gpt-oss-20b:groq
```

---

# ğŸ“ **Estructura del Proyecto (resumen)**

```
app/
 â”œâ”€â”€ api/
 â”‚    â”œâ”€â”€ routes_rag.py
 â”œâ”€â”€ rag/
 â”‚    â”œâ”€â”€ service.py
 â”‚    â”œâ”€â”€ llm_client.py
 â”‚    â”œâ”€â”€ persons_client.py
 â”‚    â”œâ”€â”€ config.py
 â”‚    â””â”€â”€ utils.py
 â”œâ”€â”€ main.py
Dockerfile
.env
README.txt
```

---

# ğŸ§ª **Prueba rÃ¡pida**

```
curl http://localhost:8000/check_llm
```

o vÃ­a navegador:

ğŸ‘‰ http://localhost:8000/docs

---

# ğŸ¤ **Soporte**

Si deseas agregar nuevas fuentes de datos o cambiar de modelo, puedo ayudarte con la extensiÃ³n del proyecto.
